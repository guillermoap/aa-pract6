{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 5.2 - Damas chinas con Redes Neuronales\n",
    "\n",
    "### Grupo 6:\n",
    "     - Guillermo Aguirre  C.I. 4817028-5\n",
    "     - Bruno González C.I. 4815697-6\n",
    "     - Mauricio Irace C.I. 4924714-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En anteriores entregas se hicieron dos jugadores, uno entrenado contra un jugador que movia al azar, y otro que aprendio jugando contra el resultado del entrenamiento anterior. \n",
    "\n",
    "En esta entrega, se repetira el procedimiento, pero utilizando redes neuronales en vez de una lineal para calcular la función $V:Tablero\\rightarrow[-1, 1]$, que determina el valor de un tablero\n",
    "\n",
    "El éxito del aprendizaje se mide, al igual que en la primera tarea, haciendolo competir con versiones anteriores del mismo. Por supuesto, en este caso, hablamos de los dos jugadores de la primera tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Arquitectura de Red Neuronal\n",
    "\n",
    "Se utilizo como entrada de la red neuronal los mismos cuatro valores que en la primer entrega:\n",
    "\n",
    "$$ (x_1, x_2, x_3, x_4) $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$x1_=$ Cantidad de fichas propias en la zona objetivo\n",
    "\n",
    "$x2_=$ Cantidad de fichas del contrincante en la zona objetivo\n",
    "\n",
    "$x_3=$ Suma de la distancias euclideas de todas las fichas propias al objetivo\n",
    "\n",
    "$x_4=$ Suma de la distancias euclideas de todas las fichas contrincantes al objetivo\n",
    "\n",
    "Claramente, los valores de $ B_{good} = (x_1, x_4)$ favorecen al jugador, mientras que los de $ B_{bad} = (x_2, x_3)$ representan variables a minimizar. \n",
    "\n",
    "    En ese sentido, se opto por hacer dos subredes, cada una recibe una de las dos tuplas, la envía por una capa densa utilizando relu como función de activación, y luego tanh, llevando al deseado intervalo de salida [-1, 1] Los resultados se combinan restando, y se le vuelve a aplicar tangente hiperbólica para normalizar.\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Entrenamiento\n",
    "En principio no se obtuvieron buenos resultados. El jugador no sabia realmente que hacer, y no salía de su zona. \n",
    "\n",
    "A continuación decidimos pre entrenar la red, enseñandole casos de perdida y victoria.\n",
    "\n",
    "Se utilizo además el concepto de tasa de exploración, que se va reduciendo hasta un mínimo a medida que el jugador aprende. A mayor esta tasa, mayor la probabildiad de efectuar jugadas al azar, que sirven para nuevas estrategias, al costo de arriesgarse a un peor desempeño.\n",
    "\n",
    "Gracias a lo expuesto en los dos párrafos anteriores, pudimos lograr que el jugador realmente se moviera y aprendiera.\n",
    "\n",
    "Dado que en principio el jugador resultaba lento por obvias razones, se prefirio optar utilizar un limite alto de jugadas (que terminó en los 3000) y un limite pequeño de partidas para evitar overfiting (en su última versión se utilizo un valor de 10, pero 5 partidas dieron resultados similares). Cada 5 partidas, se reemplazaba el jugador anterior (en principio aleatorio) por el entrenado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lo anterior, vemos mejores resultados en el jugador que aprendió en base al aleatorio. Creemos que algunos parámetros podrían haber influido en esto, que no era lo esperado. El hecho de aprender contra el mismo rival en el segundo caso, en vez de mejorar al mismo con lo entrenado, pudo haber evitado una convergencia a un mejor valor de los pesos. \n",
    "\n",
    "No logramos llegar a los resultados que hubieramos deseado o intuido que debería pasar. Un jugador más entrenado no necesariamente juega mejor que uno menos entrenado. Para mejorar esto podríamos evaluar los atributos utilizados para evaluar los diferentes tableros, los parámetros utilizados y también el uso de técnicas más avanzadas como enfriamiento. \n",
    "\n",
    "Otra cosa a tener en cuenta es la forma de entrenamiento, que es totalmente indirecta, tal vez hubiera sido conveniente que en algunas situaciones se requiriera la intervención externa de un usuario experto que indicara al jugador lo que debería hacer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
